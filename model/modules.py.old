import torch
import torch.nn as nn
import torch.nn.functional as F

import matplotlib.pyplot as plt


def get_top_k(x, k=10, mask_type="pass_through", topk_dim=1, scatter_dim=1):
    """Returns a tensor of 0's and top k values and applies a mask
    
    Args:
        x: (tensor) input.
        k: (int) how many top k examples to pass through.
        mask_type: (string) Options: ['pass_through', 'hopfield', 'binary']
    """

    zeros = torch.zeros_like(x)
    vals, idx = torch.topk(x, k, dim=topk_dim)  #[32, 121, 28, 28]
    # print(zeros.size())
    # exit()
    top_ks = zeros.scatter(scatter_dim, idx, vals)

    if mask_type != "pass_through":
        if mask_type == "binary":
            # Converts values to 1, 0
            top_ks[top_ks != 0] = 1
        elif mask_type == "hopfield":
            # Converts values to 1, -1
            top_ks[top_ks != 0] = 1
            top_ks[top_ks < 1] = -1
        else:
            raise Exception(
                'Valid options: "pass", "hopfield" (-1, 1), or "binary" (0, 1)')

    return top_ks


def bipolarize(x):
    # zero center
    x -= torch.mean(x, dim=0, keepdim=True)
    x /= torch.std(x, dim=0, keepdim=False)
    x[x > 0] = 1
    x[x < 1] = -1

    return x


class CA1(nn.Module):
    """Reconstructs the inputs that originated from EC network.

    Consists of 2 fully connected layers, recieving inputs from CA3
    and outputs to EC. 
    """

    def __init__(self):
        super(CA1, self).__init__()
        self.fc1 = nn.Linear(225, 100)
        self.fc2 = nn.Linear(100, 2704)  #2704 = 52 * 52

    def forward(self, x):
        x = torch.flatten(x)
        x = F.leaky_relu(self.fc1(x), 0.01)
        x = F.leaky_relu(self.fc2(x), 0.01)
        return x


class CA3(nn.Module):
    """CA3 is a Hopfield network.
    """

    def __init__(self, D_in):
        super(CA3, self).__init__()
        self.W = torch.zeros(D_in)

    def forward(self, x):
        """
        """
        x = bipolarize(x)
        print(x[0])
        exit()
        # Create matrix
        x = torch.sign(torch.matmul(x.T, x))
        x = x - torch.eye(225)
        self.W = self.W + x
        print(self.W)
        return self.W

    # def retrieve(self, y):
    # Assuming that the network has been trained...
    # Initialize network with initial pattern.
    # y_i(0) = x_i # Where 0 <= i <= N - 1

    # iterate until convergence:
    # y_i(t + 1) = theta * (w * y)

    # Energy formula
    # y as vectors = y.T * y * W
    # E = -0.5 * y.T * W * y # Converges when E doesn't significantly change anymore.


class DG(nn.Module):
    """Dentate Gyrus network"""

    def __init__(self, N, D_in, D_out, k):
        super(DG, self).__init__()
        self.fc1 = nn.Linear(D_in, D_out)
        # Initialize uniform distribution
        nn.init.xavier_uniform_(self.fc1.weight)

        # Initialize inhibition factor
        self.phi = InhibitionMask(N, D_out)

    def forward(self, x):
        """
        Args:
            x: (tensor) sizedtorch.Size([32, 225])
        """

        x = F.leaky_relu(self.fc1(x))  # [32, 225]

        for i in range(len(x[:,])):
            s = x[i, :]
            s = s.clone() * self.phi().clone()
            s = get_top_k(s, 10, mask_type="binary")
            self.phi.update(s, 0.1618)
            x[i, :] = s

        return x


class InhibitionMask():
    """Applies mask of inhibition to a layer
    """

    def __init__(self, N, dim):
        super(InhibitionMask, self).__init__()
        # set firstdim to 1 so it works on a sample not a batch
        self.phi = torch.ones(1, dim)

    def update(self, top_k_mask, gamma=1):
        """
        Args:
            gamma: (float) range: 0-1. Amount to decrease inhibition each timestep.
        """

        self.phi[self.phi < 1] += gamma
        self.phi[self.phi >= 1] = 1.0

        # Elementwise mul by (1 - top k)
        # Example: 1 - 1 = 0, making active top_k index = 0 in mask.
        self.phi = self.phi * (1 - top_k_mask)

    def reset(self):
        self.phi = torch.ones(10)

    def __call__(self):
        return self.phi


class ECPretrain(nn.Module):
    """Pre-training... conducted on background split.

    Sparse convolutional autoencoder, develops filters that detecct a set of
            primitive visual concepts that consist of straight and curved edges, sometimes with junctions.

    Alphabets: 20
    Batches: 2000
    Batch size: 128
    """

    def __init__(self, D_in, D_out, KERNEL_SIZE, STRIDE):
        """ 
        Args:
            D_in: (int) input channels, black and white, therefore 1. 
            D_out: (int) filter count, as per paper, 121
            KERNEL_SIZE: (int) 7 because ((52 - 7) / 5) + 1 = 10
                note: the paper calls for 10, but not sure what the basis
                      for using an even kernel size is. Will ask. 
            STRIDE: (int) set to 5
        """

        super(ECPretrain, self).__init__()
        self.encoder = nn.Conv2d(D_in,
                                 D_out,
                                 kernel_size=KERNEL_SIZE,
                                 stride=STRIDE,
                                 padding=0)
        self.decoder = nn.Conv2d(D_out, 1, kernel_size=1, stride=1, padding=0)

    def forward(self, x):
        """
        Todo:
            Insert hooks
        """
        x = self.encoder(x)  # Size: [64, 121, 10, 10}
        x = get_top_k(x, 1)  # Size: [64, 121, 10, 10]
        x = F.interpolate(x, 52, mode="nearest")
        x = self.decoder(x)  # Desired size: [64, 1, 52, 52]

        return torch.sigmoid(x)


class EC(nn.Module):
    """Standard EC with decoder removed and maxpool added.  
    """

    def __init__(self, N, D_in, D_out, KERNEL_SIZE, STRIDE):
        super(EC, self).__init__()
        self.encoder = nn.Conv2d(D_in,
                                 D_out,
                                 kernel_size=KERNEL_SIZE,
                                 stride=STRIDE,
                                 padding=2)
        self.N = N

    def forward(self, x):
        """
        Todo:
            Insert hooks
        """
        x = self.encoder(x)
        x = get_top_k(x, 1)
        x = F.max_pool2d(x, 2, 2)
        x = x.view(self.N, -1)
        return x


class ECToCA3(nn.Module):

    def __init__(self, D_in):
        super(ECToCA3, self).__init__()
        self.fc1 = nn.Linear(D_in, 800)
        self.fc2 = nn.Linear(800, 225)

    def forward(self, x):
        x = F.leaky_relu(self.fc1(x), 0.1)
        x = torch.sigmoid(self.fc2(x))
        return x
